{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisticScore.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AjeetSingh02/Notebooks/blob/master/LogisticScore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nQd_3QelEfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import average_precision_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKMDgxtqhJd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticScore:   \n",
        "    def __init__(self, ytrue, ypred, dtype=\"binary\", metric=None, sample_weight=None, adjusted=False, \n",
        "                 labels=None, weights=None, average='macro', pos_label=1, normalize=True):\n",
        "        self.y_true = ytrue\n",
        "        self.y_pred = ypred\n",
        "        self.type_of_data = dtype\n",
        "        self.metric = metric\n",
        "        self.sample_weight = sample_weight\n",
        "        self.adjusted = adjusted\n",
        "        self.labels = labels\n",
        "        self.weights = weights\n",
        "        self.average = average\n",
        "        self.pos_label = pos_label\n",
        "        self.normalize = normalize\n",
        "\n",
        "     \n",
        "    #binary and multiclass\n",
        "    def balanced_accuracy_score(self):\n",
        "        return balanced_accuracy_score(self.y_true, self.y_pred, self.sample_weight, self.adjusted) \n",
        "    \n",
        "    #binary and multiclass\n",
        "    def cohen_kappa_score(self):\n",
        "        return cohen_kappa_score(self.y_true, self.y_pred, self.labels, self.weights, self.sample_weight)\n",
        "    \n",
        "    #binary and multiclass\n",
        "    def confusion_matrix(self):\n",
        "        return confusion_matrix(self.y_true, self.y_pred, self.labels, self.sample_weight)\n",
        "    \n",
        "    #binary and multiclass\n",
        "    def matthews_corrcoef(self):\n",
        "        return matthews_corrcoef(self.y_true, self.y_pred, self.sample_weight)\n",
        "    \n",
        "    #binary and multilabel\n",
        "    def average_precision_score(self):\n",
        "        return average_precision_score(self.y_true, self.y_pred, self.average, self.pos_label, self.sample_weight)\n",
        "    \n",
        "    #binary, multiclass & multilabel\n",
        "    def accuracy_score(self):\n",
        "        return accuracy_score(self.y_true, self.y_pred, self.normalize, self.sample_weight)\n",
        "    \n",
        "    #binary, multiclass & multilabel\n",
        "    def f1_score(self):\n",
        "        return f1_score(self.y_true, self.y_pred, self.labels, self.pos_label, self.average, self.sample_weight)\n",
        "    \n",
        "    #binary, multiclass & multilabel\n",
        "    def precision_score(self):\n",
        "        return precision_score(self.y_true, self.y_pred, self.labels, self.pos_label, self.average, self.sample_weight)\n",
        "    \n",
        "    #binary, multiclass & multilabel\n",
        "    def recall_score(self):\n",
        "        return recall_score(self.y_true, self.y_pred, self.labels, self.pos_label, self.average, self.sample_weight)\n",
        "    \n",
        "    def evaluate_score(self):\n",
        "        #binary and multiclass\n",
        "        if self.type_of_data == \"binary\" and self.metric in (None, \"balanced_accuracy_score\"):\n",
        "            return self.balanced_accuracy_score()\n",
        "\n",
        "        elif self.type_of_data == \"multiclass\" and self.metric == \"balanced_accuracy_score\":\n",
        "          return self.balanced_accuracy_score()\n",
        "        \n",
        "        elif self.type_of_data in (\"binary\", \"multiclass\") and self.metric == \"cohen_kappa_score\":\n",
        "          return self.cohen_kappa_score()\n",
        "        \n",
        "        elif self.type_of_data in (\"binary\", \"multiclass\") and self.metric == \"confusion_matrix\":\n",
        "            return self.confusion_matrix()\n",
        "        \n",
        "        elif self.type_of_data in (\"binary\", \"multiclass\") and self.metric == \"matthews_corrcoef\":\n",
        "            return self.matthews_corrcoef()\n",
        "        \n",
        "        #binary and multilabel. Multiclass not supported\n",
        "        elif self.type_of_data in (\"binary\", \"multilabel\") and self.metric == \"average_precision_score\":\n",
        "            return self.average_precision_score()\n",
        "        \n",
        "        #binary, multiclass and multilabel\n",
        "        elif self.type_of_data in (\"binary\", \"multiclass\", \"multilabel\") and self.metric == \"accuracy_score\":\n",
        "            return self.accuracy_score()\n",
        "        \n",
        "        elif self.type_of_data in (\"binary\", \"multiclass\", \"multilabel\") and self.metric == \"f1_score\":\n",
        "            return self.f1_score()\n",
        "        \n",
        "        elif self.type_of_data in (\"binary\", \"multiclass\", \"multilabel\") and self.metric == \"precision_score\":\n",
        "            return self.precision_score()\n",
        "        \n",
        "        elif self.type_of_data in (\"binary\", \"multiclass\", \"multilabel\") and self.metric == \"recall_score\":\n",
        "            return self.recall_score()\n",
        "        \n",
        "        else:\n",
        "            return \"wrong choice\"\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe9oaKLQraFC",
        "colab_type": "text"
      },
      "source": [
        "**Output format options:**\n",
        "1. Binary\n",
        "2. Multi-class\n",
        "3. Multi-label\n",
        "\n",
        "**Metric options:**\n",
        "\n",
        "*Binary and Multi-class*\n",
        "1. balanced_accuracy_score \n",
        "2. cohen_kappa_score\n",
        "3. confusion_matrix\n",
        "4. matthews_corrcoef\n",
        "\n",
        "*Binary and Multi-label*\n",
        "5. average_precision_score\n",
        "\n",
        "*Binary, Multi-class and Multi-label*\n",
        "6. accuracy_score\n",
        "7. f1_score\n",
        "8. precision_score\n",
        "9. recall_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlSjpNRBhXXH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2fd29039-ff4d-42bc-bd65-5d0da286d0bf"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#binary\n",
        "ytrue = [0, 1, 0, 0, 1, 0]\n",
        "ypred = [0, 1, 0, 0, 0, 1]\n",
        "\n",
        "#multiclass\n",
        "ytrue = [0, 3, 0, 0, 1, 2]\n",
        "ypred = [0, 1, 2, 0, 3, 1]\n",
        "\n",
        "#multilabel\n",
        "ytrue = np.array([[0, 1], [1, 1]]) \n",
        "ypred = np.ones((2, 2))\n",
        "\n",
        "                 \n",
        "ls = LogisticScore(ytrue, ypred, dtype=\"binary\", metric=\"f1_score\", sample_weight=None, adjusted=False, labels=None, \n",
        "          weights=None, average='macro', pos_label=1, normalize=True)\n",
        "\n",
        "ls.evaluate_score()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    }
  ]
}